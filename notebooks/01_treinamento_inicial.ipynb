{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72bdaed",
   "metadata": {},
   "source": [
    "# 1. Project Overview\n",
    "\n",
    "This notebook fine-tunes a small Portuguese BERT model for Named Entity Recognition (NER) on Brazilian legal texts using the LeNER-Br dataset. It supports Deliverable 1 of IF1015 - Advanced Topics in Information Systems 6 (application definition plus partial training results). The workflow below keeps the full training pipeline intact while improving documentation and clarity."
   ]
  },
  {
   "cell_type": "code",
   "id": "e86711b9",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional workspace setup. Uncomment to set HF_TOKEN here if it is not already exported.\n",
    "# import os\n",
    "# os.environ[\"HF_TOKEN\"] = \"<your-hf-token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0d250",
   "metadata": {},
   "source": [
    "# 2. Dataset: LeNER-Br (Brazilian Legal NER)\n",
    "\n",
    "LeNER-Br contains Brazilian court decisions annotated with entities such as PERSON, ORGANIZATION, LOCATION, TIME, LEGISLACAO, and JURISPRUDENCIA. The dataset is loaded from Hugging Face (`peluz/lener_br`) and comes with train, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "id": "2a5d4d08",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51edf38",
   "metadata": {},
   "source": [
    "## Load the dataset and inspect splits\n",
    "\n",
    "Authenticate with `HF_TOKEN` if the dataset requires a gated download. The next cells show a sample and basic sentence-length statistics to guide the tokenization setup."
   ]
  },
  {
   "cell_type": "code",
   "id": "e97460f1",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_id = \"peluz/lener_br\"\n",
    "\n",
    "# Attempt to read the Hugging Face token from the environment (required if the dataset is gated).\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "lener = load_dataset(dataset_id, token=hf_token)\n",
    "print(lener)\n",
    "\n",
    "split_sizes = {split: len(lener[split]) for split in lener}\n",
    "for split, size in split_sizes.items():\n",
    "    print(f\"{split}: {size} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6397b233",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preview a single annotated example\n",
    "print(lener[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3203f",
   "metadata": {},
   "source": [
    "### Sentence-length statistics\n",
    "\n",
    "Quick descriptive stats to choose a reasonable maximum sequence length for BERT."
   ]
  },
  {
   "cell_type": "code",
   "id": "d59fd618",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "sizes = [len(sample[\"tokens\"]) for sample in lener[\"train\"]]\n",
    "\n",
    "avg_len = sum(sizes) / len(sizes)\n",
    "max_len = max(sizes)\n",
    "\n",
    "print(f\"Average tokens per sentence: {avg_len:.2f}\")\n",
    "print(f\"Max sentence length: {max_len}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(sizes, vert=True, showmeans=True)\n",
    "plt.title(\"Sentence length distribution (train)\")\n",
    "plt.ylabel(\"Number of tokens\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081a04d",
   "metadata": {},
   "source": [
    "# 3. Label Space and Task Definition\n",
    "\n",
    "LeNER-Br uses BIO tags for legal entities (e.g., PERSON, ORGANIZATION, LOCATION, TIME, LEGISLACAO, JURISPRUDENCIA) plus the outside tag `O`. The mapping below is reused for the token-classification head."
   ]
  },
  {
   "cell_type": "code",
   "id": "9fca6a52",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ner_feature = lener[\"train\"].features[\"ner_tags\"]\n",
    "label_list = ner_feature.feature.names\n",
    "num_labels = len(label_list)\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "print(label_list)\n",
    "print(f\"Number of labels: {num_labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d73ee",
   "metadata": {},
   "source": [
    "# 4. Model: Portuguese BERT (small LM)\n",
    "\n",
    "We fine-tune `neuralmind/bert-base-portuguese-cased` for token classification. The `Trainer` uses `seqeval` to report precision, recall, F1, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "id": "3ef94f2f",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def align_predictions(predictions, labels):\n",
    "    pred_ids = np.argmax(predictions, axis=-1)\n",
    "    true_labels, true_preds = [], []\n",
    "    for pred_sequence, label_sequence in zip(pred_ids, labels):\n",
    "        aligned_labels = []\n",
    "        aligned_preds = []\n",
    "        for pred_id, label_id in zip(pred_sequence, label_sequence):\n",
    "            if label_id == -100:\n",
    "                continue\n",
    "            aligned_labels.append(label_list[label_id])\n",
    "            aligned_preds.append(label_list[pred_id])\n",
    "        true_labels.append(aligned_labels)\n",
    "        true_preds.append(aligned_preds)\n",
    "    return true_preds, true_labels\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    true_preds, true_labels = align_predictions(predictions, labels)\n",
    "    results = metric.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2941ed0",
   "metadata": {},
   "source": [
    "# 5. Tokenization and Label Alignment\n",
    "\n",
    "Word-level labels are aligned to subword tokens. Subtokens repeat the parent word label while padding tokens use `-100` so they are ignored by the loss. `MAX_LEN` is set based on the sentence-length stats above."
   ]
  },
  {
   "cell_type": "code",
   "id": "6780be0c",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "MAX_LEN = 256\n",
    "\n",
    "def tokenize_and_align(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        prev_word = None\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != prev_word:\n",
    "                label_ids.append(labels[word_id])\n",
    "            else:\n",
    "                # Repeat the same label on subtokens to keep alignment across pieces.\n",
    "                label_ids.append(labels[word_id])\n",
    "            prev_word = word_id\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_ds = lener.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\", \"ner_tags\", \"id\"],\n",
    ")\n",
    "\n",
    "print(tokenized_ds[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c00cd",
   "metadata": {},
   "source": [
    "# 6. Training Configuration\n",
    "\n",
    "Hyperparameters for the initial run (Deliverable 1): learning rate 5e-5, 3 epochs, batch size 8/8, weight decay 0.01, mixed precision on GPU (`fp16`), max sequence length 256. Evaluation and checkpoints run at each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "91b8cd43",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results_lenerbr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    fp16=use_fp16,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_predictions = trainer.predict(tokenized_ds[\"validation\"])\n",
    "true_preds, true_labels = align_predictions(\n",
    "    eval_predictions.predictions, eval_predictions.label_ids\n",
    ")\n",
    "label_metrics = metric.compute(predictions=true_preds, references=true_labels)\n",
    "core_metrics = {\n",
    "    \"precision\": label_metrics[\"overall_precision\"],\n",
    "    \"recall\": label_metrics[\"overall_recall\"],\n",
    "    \"f1\": label_metrics[\"overall_f1\"],\n",
    "    \"accuracy\": label_metrics[\"overall_accuracy\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73fd4a",
   "metadata": {},
   "source": [
    "# 7. Initial Training Results (Deliverable 1)\n",
    "\n",
    "Aggregate validation metrics and per-entity scores are printed below for quick reference."
   ]
  },
  {
   "cell_type": "code",
   "id": "01caeebe",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Validation metrics (seqeval):\")\n",
    "for name, value in core_metrics.items():\n",
    "    print(f\"  {name.capitalize():<10}: {value:.4f}\")\n",
    "\n",
    "entity_rows = [\n",
    "    (label, scores[\"precision\"], scores[\"recall\"], scores[\"f1\"], scores[\"number\"])\n",
    "    for label, scores in label_metrics.items()\n",
    "    if not label.startswith(\"overall_\")\n",
    "]\n",
    "\n",
    "if entity_rows:\n",
    "    print(\"\\nPer-entity scores:\")\n",
    "    header = f\"{'Label':<18} {'P':>6} {'R':>6} {'F1':>6} {'N':>6}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for label, p, r, f1, n in entity_rows:\n",
    "        print(f\"{label:<18} {p:6.3f} {r:6.3f} {f1:6.3f} {n:6d}\")\n",
    "\n",
    "save_dir = \"models/lenerbr_bert_base\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"\\nModel and tokenizer saved to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fdab7",
   "metadata": {},
   "source": [
    "# 8. Qualitative Analysis: Example Predictions\n",
    "\n",
    "A single test sentence is decoded to illustrate token-level predictions after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "id": "6b5b277d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "example = lener[\"test\"][0]\n",
    "tokens = example[\"tokens\"]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    tokens,\n",
    "    is_split_into_words=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "word_ids = encoding.word_ids()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "\n",
    "pred_ids = outputs.logits.argmax(-1).cpu().numpy()[0]\n",
    "\n",
    "pred_labels = []\n",
    "clean_tokens = []\n",
    "used = set()\n",
    "\n",
    "for idx, word_id in enumerate(word_ids):\n",
    "    if word_id is None:\n",
    "        continue\n",
    "    if word_id in used:\n",
    "        continue\n",
    "    used.add(word_id)\n",
    "    clean_tokens.append(tokens[word_id])\n",
    "    pred_labels.append(label_list[pred_ids[idx]])\n",
    "\n",
    "print(\"Example sentence:\")\n",
    "print(\" \".join(clean_tokens))\n",
    "\n",
    "print(\"\\nToken-level predictions:\")\n",
    "print(f\"{'Token':<20} {'Predicted tag'}\")\n",
    "print(\"-\" * 34)\n",
    "for token, label in zip(clean_tokens, pred_labels):\n",
    "    print(f\"{token:<20} {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb23b7",
   "metadata": {},
   "source": [
    "# 9. Next Steps: Robustness, Interpretability, Adversarial Attacks\n",
    "\n",
    "Planned follow-ups: robustness checks (noise/context perturbations), interpretability analyses (attention/explanations), and adversarial stress-testing for the NER model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}