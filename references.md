## References

### Adversarial Examples and Robustness (Deep Learning)

* Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2014).
  **Intriguing properties of neural networks**. *International Conference on Learning Representations (ICLR)*.
  [https://arxiv.org/abs/1312.6199](https://arxiv.org/abs/1312.6199)

* Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015).
  **Explaining and harnessing adversarial examples**. *International Conference on Learning Representations (ICLR)*.
  [https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572)

* Su, J., Vargas, D. V., & Sakurai, K. (2019).
  **One pixel attack for fooling deep neural networks**. *IEEE Transactions on Evolutionary Computation, 23*(5), 828–841.
  [https://arxiv.org/abs/1710.08864](https://arxiv.org/abs/1710.08864)

* Huang, S., Papernot, N., Goodfellow, I., Duan, Y., & Abbeel, P. (2017).
  **Adversarial attacks on neural network policies**. *International Conference on Learning Representations (ICLR)*.
  [https://arxiv.org/abs/1702.02284](https://arxiv.org/abs/1702.02284)

* Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., … Song, D. (2018).
  **Robust physical-world attacks on deep learning visual classification**. *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.
  [https://arxiv.org/abs/1707.08945](https://arxiv.org/abs/1707.08945)

---

### Interpretability and Explanation Methods (XAI)

* Ribeiro, M. T., Singh, S., & Guestrin, C. (2016).
  **“Why should I trust you?”: Explaining the predictions of any classifier**. *ACM SIGKDD*.
  [https://arxiv.org/abs/1602.04938](https://arxiv.org/abs/1602.04938)

* Sundararajan, M., Taly, A., & Yan, Q. (2017).
  **Axiomatic attribution for deep networks**. *International Conference on Machine Learning (ICML)*.
  [https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)

* Simonyan, K., Vedaldi, A., & Zisserman, A. (2014).
  **Deep inside convolutional networks: Visualising image classification models and saliency maps**. *arXiv preprint*.
  [https://arxiv.org/abs/1312.6034](https://arxiv.org/abs/1312.6034)

* Jain, S., & Wallace, B. C. (2019).
  **Attention is not explanation**. *NAACL*.
  [https://arxiv.org/abs/1902.10186](https://arxiv.org/abs/1902.10186)

---

### Robustness and NLP-Specific Attacks

* Ebrahimi, J., Rao, A., Lowd, D., & Dou, D. (2018).
  **HotFlip: White-box adversarial examples for text classification**. *ACL*.
  [https://arxiv.org/abs/1712.06751](https://arxiv.org/abs/1712.06751)

* Belinkov, Y., & Bisk, Y. (2018).
  **Synthetic and natural noise both break neural machine translation**. *ICLR*.
  [https://arxiv.org/abs/1711.02173](https://arxiv.org/abs/1711.02173)

* Jin, D., Jin, Z., Zhou, J. T., & Szolovits, P. (2020).
  **Is BERT really robust? A strong baseline for natural language attack on text classification and entailment**. *AAAI*.
  [https://arxiv.org/abs/1907.11932](https://arxiv.org/abs/1907.11932)

---

### Legal NLP and Named Entity Recognition (NER)

* Chalkidis, I., Fergadiotis, M., Malakasiotis, P., Aletras, N., & Androutsopoulos, I. (2020).
  **Legal-BERT: The Muppets straight out of law school**. *Findings of EMNLP*.
  [https://arxiv.org/abs/2010.02559](https://arxiv.org/abs/2010.02559)

---

### Multimedia and Educational Resources (IA / NLP / NER)

* **Podcast** – Discussion on NER and NLP robustness
  [https://open.spotify.com/episode/53CQGY0NymacGQQ5LgKaPs](https://open.spotify.com/episode/53CQGY0NymacGQQ5LgKaPs)

* **YouTube** – Class on Named Entity Recognition
  [https://www.youtube.com/watch?v=nqY3xL99y-U](https://www.youtube.com/watch?v=nqY3xL99y-U)

* **YouTube Playlist** – NLP & NER fundamentals
  [https://www.youtube.com/playlist?list=PL2VXyKi-KpYs1bSnT8bfMFyGS-wMcjesM](https://www.youtube.com/playlist?list=PL2VXyKi-KpYs1bSnT8bfMFyGS-wMcjesM)

* **YouTube** – Robustness, adversarial attacks and generalization in deep learning
  [https://www.youtube.com/watch?v=61Jxw1eXQ9Y](https://www.youtube.com/watch?v=61Jxw1eXQ9Y)
